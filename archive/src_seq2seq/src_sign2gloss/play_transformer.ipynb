{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model\n",
    "\n",
    "The Jupyter Notebook aims to train a transformer model for translating Hong Kong Sign Languages into a list of glosses.\n",
    "\n",
    "## To Readers:\n",
    "\n",
    "## Notes\n",
    "- Batch size of 32 is **HARDWARE LIMIT**. On project machines, a tensor with batch size of 64 cannot be created.\n",
    "- Cache should NOT be put on the mounted windows drive. Otherwise, it would take eternity to read the keypoint files.\n",
    "    - Yes, I know it takes up C drive spaces, but I have no choice. Im not going to have 3 hours per Epoch.\n",
    "\n",
    "List of trials:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"train\" # train |dev\n",
    "\n",
    "# set to True if you want to cache Y values extracted from the split file\n",
    "CACHE_Y = True\n",
    "\n",
    "# set to True if you want to pre-generate and cache the batched data for training. \n",
    "CACHE_BATCH = False\n",
    "\n",
    "# set to True if you want to use cached batch data to train. this will cause every epoch to train from the same data\n",
    "# if you wish to train from transformed data every epoch, set this to False. this will replace the data input of model fitting process with a generator\n",
    "USE_CACHE_BATCH = True\n",
    "\n",
    "# set to True if you want to apply weighting while generating\n",
    "GENERATE_WEIGHT = False\n",
    "\n",
    "# set to True if you want to apply weighting to cache data. use if you have generated non-weighted data\n",
    "USE_WEIGHT = False\n",
    "\n",
    "# set to True if you do not want to use transformation. applies to cache data only\n",
    "NO_TRANSFORM = False\n",
    "\n",
    "# model parameters config\n",
    "HEAD_SIZE = 16\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "D_MODEL = 512\n",
    "D_FF = 512\n",
    "\n",
    "EPOCH = 128\n",
    "\n",
    "weighted_suffix = \"weighted\" if GENERATE_WEIGHT or USE_WEIGHT else \"\"\n",
    "\n",
    "# linux path\n",
    "# MODEL_DIR = f\"../model/trans_{MODE}_{D_MODEL}_{HEAD_SIZE}_{NUM_LAYERS}_{D_FF}_{DROPOUT}_{weighted_suffix}\"\n",
    "# RESULT_DIR = f\"../results/trans_{MODE}_{D_MODEL}_{HEAD_SIZE}_{NUM_LAYERS}_{D_FF}_{DROPOUT}_{weighted_suffix}\"\n",
    "# CACHE_DIR = f\"../dataset/tvb-hksl-news/keypoint_mediapipe_feat_select/\" # dir structure: {CACHE_DIR}/{date}/{name}.npy\n",
    "\n",
    "# windows pathh\n",
    "MODEL_DIR = f\"..\\\\..\\\\model\"\n",
    "RESULT_DIR = f\"..\\\\..\\\\results\\\\trans_{MODE}_{D_MODEL}_{HEAD_SIZE}_{NUM_LAYERS}_{D_FF}_{DROPOUT}_{weighted_suffix}\"\n",
    "CACHE_DIR = f\"F:\\\\dataset\\\\tvb-hksl-news\\\\keypoints_mediapipe_feat_select\" # dir structure: {CACHE_DIR}/{date}/{name}.npy\n",
    "\n",
    "# MODEL_PATH = f\"../model/train_model.keras\"\n",
    "# ENCODER_PATH = f\"../model/train_encoder.keras\"\n",
    "# DECODER_PATH = f\"../model/train_decoder.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of Save Paths and Import of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"GLOG_minloglevel\"] =\"3\"\n",
    "\n",
    "# only override for specific file generation\n",
    "make_dir_override = True\n",
    "if make_dir_override:\n",
    "    print(\"Warning: Overriding existing directories. Files may be overwritten.\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=make_dir_override)\n",
    "os.makedirs(RESULT_DIR, exist_ok=make_dir_override)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# tensorflow paths\n",
    "# MODEL_PATH = f\"{MODEL_DIR}/model.keras\"\n",
    "# ENCODER_PATH = f\"{MODEL_DIR}/encoder.keras\"\n",
    "# DECODER_PATH = f\"{MODEL_DIR}/decoder.keras\"\n",
    "\n",
    "# pytorch paths\n",
    "# MODEL_PATH = f\"{MODEL_DIR}/model.pt\"\n",
    "# ENCODER_PATH = f\"{MODEL_DIR}/encoder.pt\"\n",
    "# DECODER_PATH = f\"{MODEL_DIR}/decoder.pt\"\n",
    "\n",
    "MODEL_PATH = f\"{MODEL_DIR}\\\\model.pt\"\n",
    "ENCODER_PATH = f\"{MODEL_DIR}\\\\encoder.pt\"\n",
    "DECODER_PATH = f\"{MODEL_DIR}\\\\decoder.pt\"\n",
    "\n",
    "# RESULT_FILE_NAME = f\"{RESULT_DIR}/result.csv\"\n",
    "# HISTORY_FILE_NAME = f\"{RESULT_DIR}/history.csv\"\n",
    "# ACC_PLOT_FILE_NAME = f\"{RESULT_DIR}/acc_plot.png\"\n",
    "# LOSS_PLOT_FILE_NAME = f\"{RESULT_DIR}/loss_plot.png\"\n",
    "\n",
    "RESULT_FILE_NAME = f\"{RESULT_DIR}\\\\result.csv\"\n",
    "HISTORY_FILE_NAME = f\"{RESULT_DIR}\\\\history.csv\"\n",
    "ACC_PLOT_FILE_NAME = f\"{RESULT_DIR}\\\\acc_plot.png\"\n",
    "LOSS_PLOT_FILE_NAME = f\"{RESULT_DIR}\\\\loss_plot.png\"\n",
    "\n",
    "# print all finalized file paths\n",
    "print(\"MODEL_PATH:\", MODEL_PATH)\n",
    "print(\"ENCODER_PATH:\", ENCODER_PATH)\n",
    "print(\"DECODER_PATH:\", DECODER_PATH)\n",
    "print(\"RESULT_FILE_NAME:\", RESULT_FILE_NAME)\n",
    "print(\"HISTORY_FILE_NAME:\", HISTORY_FILE_NAME)\n",
    "print(\"ACC_PLOT_FILE_NAME:\", ACC_PLOT_FILE_NAME)\n",
    "print(\"LOSS_PLOT_FILE_NAME:\", LOSS_PLOT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras.layers import Input, Dense, Dropout, LayerNormalization\n",
    "# from keras.models import Model\n",
    "# assert len(tf.config.list_physical_devices('GPU')) > 0, \"CUDA not available. Please enable CUDA in GPU settings.\"\n",
    "# keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn import functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Parses the split files first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tvb_hksl_split_parser():\n",
    "    def __init__(self, file: str):\n",
    "        self.file = file\n",
    "        self.train_info = pd.read_csv(self.file, delimiter=\"|\") \n",
    "        # extend the dataframe with extracted information\n",
    "        self.train_info[\"glosses_tokenized\"] = self.train_info[\"glosses\"].str.split(' ')\n",
    "        # self.train_info[\"date\"] = self.train_info[\"id\"].str.split('/').apply(lambda x: x[0])\n",
    "        self.train_info[\"frames\"] = self.train_info[\"id\"].str.split('/').apply(lambda x: x[1])\n",
    "        self.train_info[\"length\"] = self.train_info[\"frames\"].str.split('-').apply(lambda x: int(x[1]) - int(x[0]) + 1)\n",
    "        # add <START> and <END> tokens to the glosses\n",
    "        self.train_info[\"glosses_tokenized\"] = self.train_info[\"glosses_tokenized\"].apply(lambda x: [\"<START>\"] + x + [\"<END>\"])\n",
    "        self.train_info[\"glosses_length\"] = self.train_info[\"glosses_tokenized\"].apply(lambda x: len(x))\n",
    "        \n",
    "\n",
    "    def get_train_id(self) -> pd.Series:\n",
    "        if os.name == \"nt\": # for windows system only\n",
    "            return self.train_info[\"id\"].str.replace(\"/\", \"\\\\\")\n",
    "        return self.train_info[\"id\"]\n",
    "\n",
    "    # def get_train_date(self) -> pd.Series:\n",
    "    #     return self.train_info[\"date\"]\n",
    "    \n",
    "    # def get_train_frames(self) -> pd.Series:\n",
    "    #     return self.train_info[\"frames\"]\n",
    "\n",
    "    # def get_train_length(self) -> pd.Series:\n",
    "    #     return self.train_info[\"length\"]\n",
    "\n",
    "    def get_train_glosses_tokenized(self) -> pd.Series:\n",
    "        return self.train_info[\"glosses_tokenized\"]\n",
    "\n",
    "    def get_max_length(self) -> int:\n",
    "        return self.train_info[\"length\"].max()\n",
    "\n",
    "    def get_max_glosses_length(self) -> int:\n",
    "        return self.train_info[\"glosses_length\"].max()\n",
    "\n",
    "    def pad_train_glosses_tokenized(self, max_length: int) -> pd.Series:\n",
    "        self.train_info[\"glosses_tokenized\"] = self.train_info[\"glosses_tokenized\"].apply(lambda x: x + [\"<END>\"] * (max_length - len(x)))\n",
    "        self.train_info[\"glosses_length\"] = self.train_info[\"glosses_tokenized\"].apply(lambda x: len(x))\n",
    "        return self.train_info[\"glosses_tokenized\"]\n",
    "    \n",
    "    def get_word_dict(self) -> dict:\n",
    "        word_dict = {}\n",
    "        for tokens in self.train_info[\"glosses_tokenized\"]:\n",
    "            for token in tokens:\n",
    "                if token not in word_dict:\n",
    "                    word_dict[token] = len(word_dict)\n",
    "        return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the word dictionary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_parser = tvb_hksl_split_parser(\"../dataset/tvb-hksl-news/split/train.csv\")\n",
    "# test_parser = tvb_hksl_split_parser(\"../dataset/tvb-hksl-news/split/test.csv\")\n",
    "# dev_parser = tvb_hksl_split_parser(\"../dataset/tvb-hksl-news/split/dev.csv\")\n",
    "\n",
    "train_parser = tvb_hksl_split_parser(r\"F:\\dataset\\tvb-hksl-news\\split\\train.csv\")\n",
    "test_parser = tvb_hksl_split_parser(r\"F:\\dataset\\tvb-hksl-news\\split\\test.csv\")\n",
    "dev_parser = tvb_hksl_split_parser(r\"F:\\dataset\\tvb-hksl-news\\split\\dev.csv\")\n",
    "\n",
    "# make a word dictionary\n",
    "word_dict = {}\n",
    "word_dict[\"<END>\"] = len(word_dict)\n",
    "word_dict[\"<START>\"] = len(word_dict)\n",
    "word_dict[\"<X>\"] = len(word_dict)\n",
    "word_dict[\"<BAD>\"] = len(word_dict)\n",
    "word_dict[\"<MUMBLE>\"] = len(word_dict)\n",
    "word_dict[\"<STOP>\"] = len(word_dict)\n",
    "# word_dict[\"<UNK>\"] = len(word_dict)\n",
    "\n",
    "for parser in [train_parser, test_parser, dev_parser]:\n",
    "    for glosses in parser.get_train_glosses_tokenized():\n",
    "        for word in glosses:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = len(word_dict)\n",
    "\n",
    "# save the word dictionary\n",
    "with open(\"../data/word_dict.json\", \"w\") as f:\n",
    "    json.dump(word_dict, f)\n",
    "# save reverse word dictionary\n",
    "reverse_word_dict = {v: k for k, v in word_dict.items()}\n",
    "with open(\"../data/reverse_word_dict.json\", \"w+\") as f:\n",
    "    json.dump(reverse_word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency map: word -> frequency\n",
    "token_freq = {}\n",
    "total_word_count = 0\n",
    "for k, v in word_dict.items():\n",
    "    token_freq[v] = 0\n",
    "for parser in [train_parser, test_parser, dev_parser]:\n",
    "    for glosses in parser.get_train_glosses_tokenized():\n",
    "        for word in glosses:\n",
    "            token_freq[word_dict[word]] += 1\n",
    "            total_word_count += 1\n",
    "print(token_freq)\n",
    "print(len(token_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a weighting list, where lower frequency words have higher weight\n",
    "\n",
    "# basic inverse frequency weighting\n",
    "# weighting_list = [1 / token_freq[word] for word in token_freq]\n",
    "# weighting_list = [x / max(weighting_list) for x in weighting_list]\n",
    "\n",
    "# tf-idf weighting\n",
    "tf_list = np.array([token_freq[word] / total_word_count for word in token_freq]) # freq / total for each word\n",
    "idf_list = np.log(len(token_freq) / tf_list) # log(total / freq ratio) for each word\n",
    "weighting_list = tf_list * idf_list \n",
    "\n",
    "# make sure that <START> and <END> have full weight\n",
    "weighting_list[word_dict[\"<START>\"]] = 1\n",
    "weighting_list[word_dict[\"<END>\"]] = 1\n",
    "print(weighting_list)\n",
    "\n",
    "# export weighting list\n",
    "# convert numpy array to list\n",
    "weighting_list = weighting_list.tolist()\n",
    "with open(\"../data/weighting_list.json\", \"w\") as f:\n",
    "    json.dump(weighting_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample preprocessing\n",
    "# train_parser.rare_sample_reduction(token_freq)\n",
    "# test_parser.rare_sample_reduction(token_freq)\n",
    "# dev_parser.rare_sample_reduction(token_freq)\n",
    "\n",
    "if MODE == \"train\":\n",
    "    actual_train_parser = train_parser\n",
    "elif MODE == \"dev\":\n",
    "    actual_train_parser = dev_parser\n",
    "\n",
    "# if a word in test_parser is not in dev_parser or train_parser, remove that sample\n",
    "# this is to prevent the model from predicting words that are not in the training set\n",
    "# supposedly, this should not happen, but just in case\n",
    "parser_word_dict = actual_train_parser.get_word_dict()\n",
    "test_parser.train_info = test_parser.train_info[test_parser.train_info[\"glosses_tokenized\"].apply(lambda x: all([word in parser_word_dict for word in x]))]\n",
    "    \n",
    "# assert that all words in test_parser are also in train_parser\n",
    "test_word_dict = test_parser.get_word_dict()\n",
    "assert all([word in parser_word_dict for word in test_word_dict])\n",
    "\n",
    "# finally, print the number of samples in each parser\n",
    "print(f\"train_parser: {len(train_parser.train_info)}\")\n",
    "print(f\"test_parser: {len(test_parser.train_info)}\")\n",
    "print(f\"dev_parser: {len(dev_parser.train_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need generate the decoder-input and encoder-input, thankfully.\n",
    "\n",
    "Still, we have to pad the list of tokens to the same length, and replace it with numbers instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the glosses to the maximum length\n",
    "train_gloss_max_length = actual_train_parser.get_max_glosses_length()\n",
    "test_gloss_max_length = test_parser.get_max_glosses_length()\n",
    "dev_gloss_max_length = dev_parser.get_max_glosses_length()\n",
    "\n",
    "print(f\"train_gloss_max_length: {train_gloss_max_length}\")\n",
    "print(f\"test_gloss_max_length: {test_gloss_max_length}\")\n",
    "print(f\"dev_gloss_max_length: {dev_gloss_max_length}\")\n",
    "\n",
    "max_gloss_length = max(train_gloss_max_length, test_gloss_max_length, dev_gloss_max_length)\n",
    "train_parser.pad_train_glosses_tokenized(max_gloss_length)\n",
    "test_parser.pad_train_glosses_tokenized(max_gloss_length)\n",
    "dev_parser.pad_train_glosses_tokenized(max_gloss_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Skipping image generation code here for now since the keypoints are cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mediapipe_keypoints_face_sublist() -> list[list[int]]:\n",
    "    # face\n",
    "    # NOTE: the following keypoint indices are HARD-CODED based on the visualization of the face mesh\n",
    "    # reference: https://github.com/LearningnRunning/py_face_landmark_helper/blob/main/mediapipe_helper/config.py\n",
    "    # image: https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "    # related stack overflow post: https://stackoverflow.com/questions/74901522/can-mediapipe-specify-which-parts-of-the-face-mesh-are-the-lips-or-nose-or-eyes\n",
    "    FACE_LIPS = [0, 267, 269, 270, 13, 14, 17, 402, 146, 405, 409, 415, 291, 37, 39, 40, 178, 308, 181, 310, 311, 312, 185, 314, 317, 318, 61, 191, 321, 324, 78, 80, 81, 82, 84, 87, 88, 91, 95, 375]\n",
    "    LEFT_EYE = [384, 385, 386, 387, 388, 390, 263, 362, 398, 466, 373, 374, 249, 380, 381, 382]\n",
    "    LEFT_EYEBROW = [293, 295, 296, 300, 334, 336, 276, 282, 283, 285]\n",
    "    RIGHT_EYE = [160, 33, 161, 163, 133, 7, 173, 144, 145, 246, 153, 154, 155, 157, 158, 159]\n",
    "    RIGHT_EYEBROW = [65, 66, 70, 105, 107, 46, 52, 53, 55, 63]\n",
    "    FACE_NOSE = [1, 2, 4, 5, 6, 19, 275, 278, 294, 168, 45, 48, 440, 64, 195, 197, 326, 327, 344, 220, 94, 97, 98, 115]\n",
    "    FACE_OVAL = [132, 389, 136, 10, 397, 400, 148, 149, 150, 21, 152, 284, 288, 162, 297, 172, 176, 54, 58, 323, 67, 454, 332, 338, 93, 356, 103, 361, 234, 109, 365, 379, 377, 378, 251, 127]\n",
    "    return [\n",
    "        FACE_LIPS, \n",
    "        LEFT_EYE, \n",
    "        LEFT_EYEBROW, \n",
    "        RIGHT_EYE, \n",
    "        RIGHT_EYEBROW, \n",
    "        # FACE_NOSE, \n",
    "        # FACE_OVAL\n",
    "    ]\n",
    "\n",
    "STATIC_FACE_KEYPOINT_INDEX = [i for sublist in get_mediapipe_keypoints_face_sublist() for i in sublist]\n",
    "\n",
    "def get_mediapipe_keypoints_index() -> list[int]:\n",
    "    POSE_UNPROCESSED = range(0, 33*4)\n",
    "    # POSE = [i for i in POSE_UNPROCESSED if i % 4 != 3]\n",
    "    # for x, y only\n",
    "    # discard Z due to documentation https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md\n",
    "    POSE = [i for i in POSE_UNPROCESSED if i % 4 != 2 and i % 4 != 3]\n",
    "\n",
    "    FACE_UNPROCESSED = [i + 33*4 for i in STATIC_FACE_KEYPOINT_INDEX]\n",
    "    # face keypoints are in x, y, z format flattened, so we need to capture all x, y, z values\n",
    "    FACE = [i for j in range(0, len(FACE_UNPROCESSED), 3) for i in range(FACE_UNPROCESSED[j], FACE_UNPROCESSED[j] + 3)]\n",
    "    # for x, y only\n",
    "    # FACE = [i for j in range(0, len(FACE_UNPROCESSED), 3) for i in range(FACE_UNPROCESSED[j], FACE_UNPROCESSED[j] + 2)]\n",
    "\n",
    "    # hands\n",
    "    LEFT_HAND = list(range(33*4 + 468*3, 33*4 + 468*3 + 21*3))\n",
    "    RIGHT_HAND = list(range(33*4 + 468*3 + 21*3, 33*4 + 468*3 + 21*3 + 21*3))\n",
    "    # for x, y only\n",
    "    # LEFT_HAND = [i for i in list(range(33*4 + 468*3, 33*4 + 468*3 + 21*3)) if i % 3 != 2]\n",
    "    # RIGHT_HAND = [i for i in list(range(33*4 + 468*3 + 21*3, 33*4 + 468*3 + 21*3 + 21*3)) if i % 3 != 2]\n",
    "    KEYPOINTS_INDEX = POSE + FACE + LEFT_HAND + RIGHT_HAND\n",
    "    return KEYPOINTS_INDEX, [POSE, FACE, LEFT_HAND, RIGHT_HAND]\n",
    "\n",
    "STATIC_KEYPOINTS_INDEX = get_mediapipe_keypoints_index()[0]\n",
    "\n",
    "def preprocess_keypoints(keypoints, angle=0, tx=0, ty=0, tz=0, scale=1):\n",
    "    POSE, FACE, LEFT_HAND, RIGHT_HAND = get_mediapipe_keypoints_index()[1]\n",
    "    pose_indices = range(len(POSE))\n",
    "    face_indices = range(len(POSE), len(POSE) + len(FACE))\n",
    "    left_hand_indices = range(len(POSE) + len(FACE), len(POSE) + len(FACE) + len(LEFT_HAND))\n",
    "    right_hand_indices = range(len(POSE) + len(FACE) + len(LEFT_HAND), len(POSE) + len(FACE) + len(LEFT_HAND) + len(RIGHT_HAND))\n",
    "    # print(POSE, FACE, LEFT_HAND, RIGHT_HAND)\n",
    "    keypoints = keypoints.copy()\n",
    "    # print(keypoints.shape)\n",
    "    pose_keypoints = keypoints[pose_indices]\n",
    "    face_keypoints = keypoints[face_indices]\n",
    "    left_hand_keypoints = keypoints[left_hand_indices]\n",
    "    right_hand_keypoints = keypoints[right_hand_indices]\n",
    "\n",
    "    # pose only has X and Y in flattened format\n",
    "    pose_keypoints = pose_keypoints.reshape(-1, 2)\n",
    "    # rotate each point by angle\n",
    "    angle = np.radians(angle)\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), -np.sin(angle)], \n",
    "        [np.sin(angle), np.cos(angle)]\n",
    "    ])\n",
    "    pose_keypoints = np.dot(pose_keypoints, rotation_matrix)\n",
    "    pose_keypoints[:, 0] += tx\n",
    "    pose_keypoints[:, 1] += ty\n",
    "    pose_keypoints[:, :2] = scale * (pose_keypoints[:, :2] - 0.5) + 0.5\n",
    "    keypoints[pose_indices] = pose_keypoints.flatten()\n",
    "\n",
    "    # other parts have X, Y, Z in flattened format\n",
    "    face_keypoints = face_keypoints.reshape(-1, 3)\n",
    "    left_hand_keypoints = left_hand_keypoints.reshape(-1, 3)\n",
    "    right_hand_keypoints = right_hand_keypoints.reshape(-1, 3)\n",
    "    angle = np.radians(angle)\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), -np.sin(angle), 0],\n",
    "        [np.sin(angle), np.cos(angle), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    face_keypoints = np.dot(face_keypoints, rotation_matrix)\n",
    "    left_hand_keypoints = np.dot(left_hand_keypoints, rotation_matrix)\n",
    "    right_hand_keypoints = np.dot(right_hand_keypoints, rotation_matrix)\n",
    "    face_keypoints[:, 0] += tx\n",
    "    face_keypoints[:, 1] += ty\n",
    "    face_keypoints[:, 2] += tz\n",
    "    left_hand_keypoints[:, 0] += tx\n",
    "    left_hand_keypoints[:, 1] += ty\n",
    "    left_hand_keypoints[:, 2] += tz\n",
    "    right_hand_keypoints[:, 0] += tx\n",
    "    right_hand_keypoints[:, 1] += ty\n",
    "    right_hand_keypoints[:, 2] += tz\n",
    "    face_keypoints[:, :2] = scale * (face_keypoints[:, :2] - 0.5) + 0.5\n",
    "    left_hand_keypoints[:, :2] = scale * (left_hand_keypoints[:, :2] - 0.5) + 0.5\n",
    "    right_hand_keypoints[:, :2] = scale * (right_hand_keypoints[:, :2] - 0.5) + 0.5\n",
    "    keypoints[face_indices] = face_keypoints.flatten()\n",
    "    keypoints[left_hand_indices] = left_hand_keypoints.flatten()\n",
    "    keypoints[right_hand_indices] = right_hand_keypoints.flatten()\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Generator\n",
    "\n",
    "Create a generator by inheriting `keras.utils.Sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation: get the largest length of sequences of x\n",
    "train_max_length = train_parser.get_max_length()\n",
    "test_max_length = test_parser.get_max_length()\n",
    "dev_max_length = dev_parser.get_max_length()\n",
    "\n",
    "X_max_length = max(train_max_length, test_max_length, dev_max_length)\n",
    "print(\"Max length of sequences of X:\", X_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation for Tensorflow\n",
    "\n",
    "# import keras\n",
    "# class CachedKeypointGenerator(keras.utils.Sequence):\n",
    "#     def __init__(self, x_dir, parser: tvb_hksl_split_parser, batch_size=32):\n",
    "#         self.x_dir = x_dir\n",
    "#         self.batch_size = batch_size\n",
    "#         self.list_x_files = sorted(os.listdir(self.x_dir))\n",
    "#         if NO_TRANSFORM:\n",
    "#             self.list_x_files = [file for file in self.list_x_files if file.startswith(\"control\")]\n",
    "#         self.total_files = len(self.list_x_files)\n",
    "#         self.batch_index = 0\n",
    "#         self.y = parser.get_train_glosses_tokenized().to_list()\n",
    "#         self.y = [[word_dict[word] for word in words] for words in self.y]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.total_files\n",
    "\n",
    "#     def __getitem__(self, idx: int):\n",
    "#         batch_x = np.load(os.path.join(self.x_dir, self.list_x_files[idx]), mmap_mode=\"r\")\n",
    "#         batch_y = self.y[idx:idx + self.batch_size]\n",
    "#         # one-hot encode the y values\n",
    "#         batch_y = keras.utils.to_categorical(batch_y, num_classes=len(word_dict))\n",
    "#         return batch_x, batch_y\n",
    "    \n",
    "# train_keypoint_generator = CachedKeypointGenerator(os.path.join(CACHE_DIR, \"x\"), train_parser)\n",
    "# first_batch = train_keypoint_generator.__getitem__(0)\n",
    "# X_shape = first_batch[0].shape\n",
    "# Y_shape = np.array(first_batch[1]).shape\n",
    "# print(first_batch[0])\n",
    "# print(first_batch[1])\n",
    "# del first_batch\n",
    "# print(type(X_shape), X_shape)\n",
    "# print(type(Y_shape), Y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation for PyTorch\n",
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, x_dir, parser: tvb_hksl_split_parser):\n",
    "        self.x_dir = x_dir\n",
    "        # path of each file is x_dir + get_train_id() + \".npy\"\n",
    "        self.list_x_files = parser.get_train_id().to_list()\n",
    "        self.total_files = len(self.list_x_files)\n",
    "\n",
    "        self.y = parser.get_train_glosses_tokenized().to_list()\n",
    "        self.y = [[word_dict[word] for word in words] for words in self.y]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_files\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        x = np.load(os.path.join(self.x_dir, self.list_x_files[idx] + \".npy\"), mmap_mode=\"r\")\n",
    "        x = preprocess_keypoints(\n",
    "            x,\n",
    "            angle=np.random.uniform(-10, 10),\n",
    "            tx=np.random.uniform(-0.1, 0.1),\n",
    "            ty=np.random.uniform(-0.1, 0.1),\n",
    "            tz=np.random.uniform(-0.1, 0.1),\n",
    "            scale=np.random.uniform(0.8, 1.2)\n",
    "        )\n",
    "        y = self.y[idx]\n",
    "        # one-hot encode the y values\n",
    "        y = torch.nn.functional.one_hot(torch.tensor(y), num_classes=len(word_dict)).float()\n",
    "        return x, y\n",
    "    \n",
    "train_keypoint_dataset = KeypointDataset(CACHE_DIR, train_parser)\n",
    "train_keypoint_loader = DataLoader(train_keypoint_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "dev_keypoint_dataset = KeypointDataset(CACHE_DIR, dev_parser)\n",
    "dev_keypoint_loader = DataLoader(dev_keypoint_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_keypoint_dataset = KeypointDataset(CACHE_DIR, test_parser)\n",
    "test_keypoint_loader = DataLoader(test_keypoint_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Create a transformer model with self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageTransformer(nn.Module):\n",
    "    def __init__(self, d_model, head_size, num_layers, d_ff, dropout, num_classes):\n",
    "        super(SignLanguageTransformer, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, head_size, d_ff, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, head_size, d_ff, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        memory = self.transformer_encoder(src)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "        output = self.fc(output)\n",
    "        return output.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # failed tensorflow model\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.losses import SparseCategoricalCrossentropy\n",
    "# from keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# # Positional Encoding\n",
    "# def positional_encoding(position, d_model):\n",
    "#     angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "#     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "#     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "#     pos_encoding = angle_rads[np.newaxis, ...]\n",
    "#     return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# def get_angles(pos, i, d_model):\n",
    "#     angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "#     return pos * angle_rates\n",
    "\n",
    "# # Scaled Dot-Product Attention\n",
    "# def scaled_dot_product_attention(q, k, v, mask):\n",
    "#     matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "#     dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "#     scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "#     if mask is not None:\n",
    "#         scaled_attention_logits += (mask * -1e9)\n",
    "#     attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "#     output = tf.matmul(attention_weights, v)\n",
    "#     return output, attention_weights\n",
    "\n",
    "# # Multi-Head Attention\n",
    "# class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads):\n",
    "#         super(MultiHeadAttention, self).__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.d_model = d_model\n",
    "#         assert d_model % self.num_heads == 0\n",
    "#         self.depth = d_model // self.num_heads\n",
    "#         self.wq = Dense(d_model)\n",
    "#         self.wk = Dense(d_model)\n",
    "#         self.wv = Dense(d_model)\n",
    "#         self.dense = Dense(d_model)\n",
    "\n",
    "#     def split_heads(self, x, batch_size):\n",
    "#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "#         return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "#     def call(self, v, k, q, mask):\n",
    "#         batch_size = tf.shape(q)[0]\n",
    "#         q = self.wq(q)\n",
    "#         k = self.wk(k)\n",
    "#         v = self.wv(v)\n",
    "#         q = self.split_heads(q, batch_size)\n",
    "#         k = self.split_heads(k, batch_size)\n",
    "#         v = self.split_heads(v, batch_size)\n",
    "#         scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask)\n",
    "#         scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "#         concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "#         output = self.dense(concat_attention)\n",
    "#         return output\n",
    "\n",
    "# # Point-wise Feed Forward Network\n",
    "# def point_wise_feed_forward_network(d_model, dff):\n",
    "#     return tf.keras.Sequential([\n",
    "#         Dense(dff, activation='relu'),\n",
    "#         Dense(d_model)\n",
    "#     ])\n",
    "\n",
    "# # Encoder Layer\n",
    "# class EncoderLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "#         super(EncoderLayer, self).__init__()\n",
    "#         self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "#         self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout1 = Dropout(rate)\n",
    "#         self.dropout2 = Dropout(rate)\n",
    "\n",
    "#     def call(self, x, training, mask):\n",
    "#         attn_output = self.mha(x, x, x, mask)\n",
    "#         attn_output = self.dropout1(attn_output, training=training)\n",
    "#         out1 = self.layernorm1(x + attn_output)\n",
    "#         ffn_output = self.ffn(out1)\n",
    "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
    "#         out2 = self.layernorm2(out1 + ffn_output)\n",
    "#         return out2\n",
    "\n",
    "# # Decoder Layer\n",
    "# class DecoderLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "#         super(DecoderLayer, self).__init__()\n",
    "#         self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "#         self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "#         self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout1 = Dropout(rate)\n",
    "#         self.dropout2 = Dropout(rate)\n",
    "#         self.dropout3 = Dropout(rate)\n",
    "\n",
    "#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "#         attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "#         attn1 = self.dropout1(attn1, training=training)\n",
    "#         out1 = self.layernorm1(attn1 + x)\n",
    "#         attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "#         attn2 = self.dropout2(attn2, training=training)\n",
    "#         out2 = self.layernorm2(attn2 + out1)\n",
    "#         ffn_output = self.ffn(out2)\n",
    "#         ffn_output = self.dropout3(ffn_output, training=training)\n",
    "#         out3 = self.layernorm3(ffn_output + out2)\n",
    "#         return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "# # Encoder\n",
    "# class Encoder(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_layers = num_layers\n",
    "#         self.embedding = Dense(d_model)\n",
    "#         self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "#         self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "#         self.dropout = Dropout(rate)\n",
    "\n",
    "#     def call(self, x, training, mask):\n",
    "#         seq_len = tf.shape(x)[1]\n",
    "#         x = self.embedding(x)\n",
    "#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "#         x += self.pos_encoding[:, :seq_len, :]\n",
    "#         x = self.dropout(x, training=training)\n",
    "#         for i in range(self.num_layers):\n",
    "#             x = self.enc_layers[i](x, training, mask)\n",
    "#         return x\n",
    "\n",
    "# # Decoder\n",
    "# class Decoder(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.num_layers = num_layers\n",
    "#         self.embedding = Embedding(target_vocab_size, d_model)\n",
    "#         self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "#         self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "#         self.dropout = Dropout(rate)\n",
    "\n",
    "#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "#         seq_len = tf.shape(x)[1]\n",
    "#         attention_weights = {}\n",
    "#         x = self.embedding(x)\n",
    "#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "#         x += self.pos_encoding[:, :seq_len, :]\n",
    "#         x = self.dropout(x, training=training)\n",
    "#         for i in range(self.num_layers):\n",
    "#             x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "#             attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "#             attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "#         return x, attention_weights\n",
    "\n",
    "# # Transformer\n",
    "# class Transformer(tf.keras.Model):\n",
    "#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "#         self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "#         self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "#         self.final_layer = Dense(target_vocab_size)\n",
    "\n",
    "#     def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "#         enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "#         dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "#         final_output = self.final_layer(dec_output)\n",
    "#         return final_output, attention_weights\n",
    "\n",
    "# # Hyperparameters\n",
    "# num_layers = 4\n",
    "# d_model = 128\n",
    "# num_heads = 8\n",
    "# dff = 512\n",
    "# input_vocab_size = X_shape[2]\n",
    "# target_vocab_size = len(word_dict)\n",
    "# pe_input = 1000\n",
    "# pe_target = 1000\n",
    "# dropout_rate = 0.1\n",
    "\n",
    "# # Create the model\n",
    "# transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate)\n",
    "\n",
    "# # Compile the model\n",
    "# transformer.compile(optimizer=Adam(learning_rate=0.001),\n",
    "#                     loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "#                     metrics=[SparseCategoricalAccuracy()])\n",
    "\n",
    "# # Summary of the model\n",
    "# transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.layers import Input, Dense, Dropout, LayerNormalization, TimeDistributed, MultiHeadAttention\n",
    "# from keras.models import Model\n",
    "\n",
    "# input_shape = (None, None, None)  # (batch_size, max_num_frames, num_features)\n",
    "# inputs = Input(shape=input_shape[1:])\n",
    "\n",
    "# # Self-attention mechanism\n",
    "# attention_output = MultiHeadAttention(num_heads=8, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "# attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "# attention_output = Dropout(0.1)(attention_output)\n",
    "\n",
    "# # Add a feed-forward network\n",
    "# ffn_output = Dense(2048, activation='relu')(attention_output)\n",
    "# ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "# ffn_output = LayerNormalization(epsilon=1e-6)(ffn_output)\n",
    "# ffn_output = Dropout(0.1)(ffn_output)\n",
    "\n",
    "# # Add a TimeDistributed layer for output\n",
    "# outputs = TimeDistributed(Dense(len(word_dict), activation='softmax'))(ffn_output) \n",
    "\n",
    "# # Define the model\n",
    "# model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = criterion.cuda()\n",
    "\n",
    "# # Initialize the optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# # Initialize the learning rate scheduler\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# # Initialize the history dictionary\n",
    "# history = {\n",
    "#     \"train_loss\": [],\n",
    "#     \"train_acc\": [],\n",
    "#     \"val_loss\": [],\n",
    "#     \"val_acc\": []\n",
    "# }\n",
    "\n",
    "# # Set model to training mode\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# for epoch in range(EPOCH):\n",
    "#     # Initialize the epoch's loss\n",
    "#     epoch_loss = 0\n",
    "#     epoch_correct = 0\n",
    "#     epoch_total = 0\n",
    "    \n",
    "#     # Train the model\n",
    "#     for i, (x, y) in enumerate(train_keypoint_generator):\n",
    "#         # Convert the data to PyTorch tensors and move to GPU\n",
    "#         x = torch.from_numpy(x).type(torch.FloatTensor).to(\"cuda\")\n",
    "#         y = torch.from_numpy(np.array(y)).type(torch.LongTensor).to(\"cuda\")\n",
    "        \n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         output = model(x, y)\n",
    "        \n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(output.view(-1, len(word_dict)), y.view(-1))\n",
    "#         epoch_loss += loss.item()\n",
    "        \n",
    "#         # Calculate the accuracy\n",
    "#         _, predicted = torch.max(output, 2)\n",
    "#         epoch_total += y.size(0) * y.size(1)\n",
    "#         epoch_correct += (predicted == y).sum().item()\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Update the weights\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Print statistics\n",
    "#         print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "#     # Calculate the epoch loss and accuracy\n",
    "#     epoch_loss /= len(train_keypoint_generator)\n",
    "#     epoch_acc = epoch_correct / epoch_total\n",
    "#     print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}, Accuracy: {epoch_acc}\")\n",
    "    \n",
    "#     # Update the history dictionary\n",
    "#     history[\"train_loss\"].append(epoch_loss)\n",
    "#     history[\"train_acc\"].append(epoch_acc)\n",
    "    \n",
    "#     # Skip validation for now\n",
    "\n",
    "# # Save the model\n",
    "# torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create masks\n",
    "\n",
    "# class MaksedKeypointGenerator(keras.utils.Sequence):\n",
    "#     def __init__(self, cached_keypoint_generator: CachedKeypointGenerator):\n",
    "#         self.cached_keypoint_generator = cached_keypoint_generator\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.cached_keypoint_generator.__len__()\n",
    "    \n",
    "#     def create_padding_mask(self, seq):\n",
    "#         seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "#         return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "#     def create_look_ahead_mask(self, size):\n",
    "#         mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "#         return mask\n",
    "    \n",
    "#     def __getitem__(self, idx: int):\n",
    "#         x, y = self.cached_keypoint_generator.__getitem__(idx)\n",
    "        \n",
    "#         # Create padding masks\n",
    "#         enc_padding_mask = self.create_padding_mask(x)\n",
    "#         dec_padding_mask = self.create_padding_mask(x)\n",
    "#         look_ahead_mask = self.create_look_ahead_mask(y.shape[1])\n",
    "\n",
    "#         return x, y, enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
    "    \n",
    "#     def on_epoch_end(self):\n",
    "#         self.cached_keypoint_generator.on_epoch_end()\n",
    "\n",
    "# train_masked_keypoint_generator = MaksedKeypointGenerator(train_keypoint_generator)\n",
    "# first_batch = train_masked_keypoint_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SignLanguageTransformer(D_MODEL, HEAD_SIZE, NUM_LAYERS, D_FF, DROPOUT, len(word_dict))\n",
    "\n",
    "# loss function: categorical crossentropy\n",
    "# optimizer: Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "# training loop\n",
    "# use dev set for validation\n",
    "num_epochs = 256\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    for i, (x, y) in enumerate(train_keypoint_loader):\n",
    "        if device == \"cuda\":\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, y)\n",
    "        loss = criterion(output.view(-1, len(word_dict)), y.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 2)\n",
    "        epoch_total += y.size(0) * y.size(1)\n",
    "        epoch_correct += (predicted == y).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}\")\n",
    "    epoch_loss /= len(train_keypoint_loader)\n",
    "    epoch_acc = epoch_correct / epoch_total\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}, Accuracy: {epoch_acc}\")\n",
    "    history[\"train_loss\"].append(epoch_loss)\n",
    "    history[\"train_acc\"].append(epoch_acc)\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    for i, (x, y) in enumerate(dev_keypoint_loader):\n",
    "        if device == \"cuda\":\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        output = model(x, y)\n",
    "        loss = criterion(output.view(-1, len(word_dict)), y.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 2)\n",
    "        epoch_total += y.size(0) * y.size(1)\n",
    "        epoch_correct += (predicted == y).sum().item()\n",
    "    epoch_loss /= len(dev_keypoint_loader)\n",
    "    epoch_acc = epoch_correct / epoch_total\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}, Accuracy: {epoch_acc}\")\n",
    "    history[\"val_loss\"].append(epoch_loss)\n",
    "    history[\"val_acc\"].append(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "# save the history\n",
    "# RESULT_FILE_NAME = f\"{RESULT_DIR}/result.csv\"\n",
    "# HISTORY_FILE_NAME = f\"{RESULT_DIR}/history.csv\"\n",
    "\n",
    "rows = []\n",
    "for i in range(num_epochs):\n",
    "    rows.append([i + 1, history[\"train_loss\"][i], history[\"train_acc\"][i], history[\"val_loss\"][i], history[\"val_acc\"][i]])\n",
    "\n",
    "import csv\n",
    "with open(HISTORY_FILE_NAME, \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Validation Loss\", \"Validation Accuracy\"])\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "result_dict = {\n",
    "    \"prediction\": [],\n",
    "    \"actual\": []\n",
    "}\n",
    "\n",
    "for i, (x, y) in enumerate(test_keypoint_loader):\n",
    "    if device == \"cuda\":\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "    output = model(x, y)\n",
    "    loss = criterion(output.view(-1, len(word_dict)), y.view(-1))\n",
    "    test_loss += loss.item()\n",
    "    _, predicted = torch.max(output, 2)\n",
    "    test_total += y.size(0) * y.size(1)\n",
    "    test_correct += (predicted == y).sum().item()\n",
    "\n",
    "    decoded_predicted = [list(word_dict.keys())[list(word_dict.values()).index(word)] for word in predicted.view(-1).cpu().numpy()]\n",
    "    decoded_actual = [list(word_dict.keys())[list(word_dict.values()).index(word)] for word in y.view(-1).cpu().numpy()]\n",
    "    result_dict[\"prediction\"].extend(decoded_predicted)\n",
    "    result_dict[\"actual\"].extend(decoded_actual)\n",
    "\n",
    "test_loss /= len(test_keypoint_loader)\n",
    "test_acc = test_correct / test_total\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "\n",
    "# save the result\n",
    "rows = []\n",
    "for i in range(len(result_dict[\"prediction\"])):\n",
    "    rows.append([result_dict[\"prediction\"][i], result_dict[\"actual\"][i]])\n",
    "\n",
    "with open(RESULT_FILE_NAME, \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Prediction\", \"Actual\"])\n",
    "    writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
