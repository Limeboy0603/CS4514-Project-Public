{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence model\n",
    "\n",
    "The Jupyter Notebook aims to train a sequence-to-sequence model for translating Hong Kong Sign Languages into a list of glosses.\n",
    "\n",
    "## To Readers:\n",
    "- You should first create a virtual environment using `requirements.txt` in this directory\n",
    "- Feature extraction takes a VERY long time to perform. If you have enough time to attempt to replicate this, you may either:\n",
    "    - Set `CACHE_BATCH` and `USE_CACHE_BATCH` to `True` in the following cell, and this notebook will cache all the keypoints; or\n",
    "    - Set `USE_CACHE_BATCH` to `False` in the following cell, and features will be extracted on the fly (takes very long!); or\n",
    "    - (CPU) Run everything before `Feature Extraction` in this notebook, then change the setting at the top of `keypoint-gen-cpu.py` and run the script; or\n",
    "    - (GPU) Run everything before `Feature Extraction` in this notebook, then change the setting at the top of `keypoint-gen-gpu.py` and run the script **on Ubuntu with GPU**. **Windows, MacOS, WSL Ubuntu, or any other Linux distributions are NOT supported** \n",
    "        - Since Holistic does not support GPU, this script uses the latest solution on face, pose and hand landmarker which includes 10 more face keypoints for the iris.\n",
    "        - Since the face contours does not utilize these 10 keypoints, they will all be filtered during generation.\n",
    "        - Thus, generated keypoints are also compatible with the intended shape of the model.\n",
    "## Notes\n",
    "- Batch size of 32 is **HARDWARE LIMIT**. On project machines, a tensor with batch size of 64 cannot be created.\n",
    "- Cache should NOT be put on the mounted windows drive. Otherwise, it would take eternity to read the keypoint files.\n",
    "    - Yes, I know it takes up C drive spaces, but I have no choice. Im not going to have 3 hours per Epoch.\n",
    "\n",
    "List of trials: (format: batchSize_Epoch_LatenDim)\n",
    "- {empty}: trained using 32_256_512, without weighting\n",
    "- weighted_32_256_512: trained using inverse frequency normalized by max outside of the notebook using a script\n",
    "- weighted_32_256_512_2: trained using inverse frequency normalized by max\n",
    "- weighted_32_256_512_3: trained using tf-idf\n",
    "    - result: improved categorical accuracy to 0.8730\n",
    "    - extended to 512 epoch: even better accuracy\n",
    "- weighted_32_512_1024_1: **we going big this time :)**. See if accuracy improves this time\n",
    "    - categorical_accuracy: 0.9268\n",
    "    - Total raw accuracy on testing data: 0.018029056537720987\n",
    "    - Average raw accuracy on testing data (method 2): 0.024452341305154224\n",
    "    - BLEU Score:  8.933399549602846e-232\n",
    "- weighted_32_512_1024_2: Same as above, but does not use weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"train\" # train |dev\n",
    "\n",
    "# set to True if you want to cache Y values extracted from the split file\n",
    "CACHE_Y = True\n",
    "\n",
    "# set to True if you want to pre-generate and cache the batched data for training. \n",
    "CACHE_BATCH = False\n",
    "\n",
    "# set to True if you want to use cached batch data to train. this will cause every epoch to train from the same data\n",
    "# if you wish to train from transformed data every epoch, set this to False. this will replace the data input of model fitting process with a generator\n",
    "USE_CACHE_BATCH = True\n",
    "\n",
    "# set to True if you want to apply weighting while generating\n",
    "GENERATE_WEIGHT = False\n",
    "\n",
    "# set to True if you want to apply weighting to cache data. use if you have generated non-weighted data\n",
    "USE_WEIGHT = False\n",
    "\n",
    "# set to True if you do not want to use transformation. applies to cache data only\n",
    "NO_TRANSFORM = False\n",
    "\n",
    "# set which RNN model to use\n",
    "RNN_MODE = \"LSTM\" # LSTM | GRU\n",
    "\n",
    "# model parameters config\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 512\n",
    "LATENT_DIM = 1024\n",
    "TRIAL = 2\n",
    "\n",
    "weighted_suffix = \"weighted\" if GENERATE_WEIGHT or USE_WEIGHT else \"\"\n",
    "\n",
    "MODEL_DIR = f\"../model/{MODE}_{RNN_MODE}_{weighted_suffix}_{BATCH_SIZE}_{EPOCH}_{LATENT_DIM}_{TRIAL}\"\n",
    "CACHE_DIR = f\"../cache/{MODE}\"\n",
    "RESULT_DIR = f\"../results/{MODE}_{RNN_MODE}_{weighted_suffix}_{BATCH_SIZE}_{EPOCH}_{LATENT_DIM}_{TRIAL}\"\n",
    "\n",
    "# MODEL_PATH = f\"../model/train_model.keras\"\n",
    "# ENCODER_PATH = f\"../model/train_encoder.keras\"\n",
    "# DECODER_PATH = f\"../model/train_decoder.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaration of Save Paths and Import of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"GLOG_minloglevel\"] =\"3\"\n",
    "\n",
    "# only override for specific file generation\n",
    "make_dir_override = True\n",
    "if make_dir_override:\n",
    "    print(\"Warning: Overriding existing directories. Files may be overwritten.\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=make_dir_override)\n",
    "os.makedirs(RESULT_DIR, exist_ok=make_dir_override)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = f\"{MODEL_DIR}/model.keras\"\n",
    "ENCODER_PATH = f\"{MODEL_DIR}/encoder.keras\"\n",
    "DECODER_PATH = f\"{MODEL_DIR}/decoder.keras\"\n",
    "\n",
    "RESULT_FILE_NAME = f\"{RESULT_DIR}/result.csv\"\n",
    "HISTORY_FILE_NAME = f\"{RESULT_DIR}/history.csv\"\n",
    "ACC_PLOT_FILE_NAME = f\"{RESULT_DIR}/acc_plot.png\"\n",
    "LOSS_PLOT_FILE_NAME = f\"{RESULT_DIR}/loss_plot.png\"\n",
    "\n",
    "# print all finalized file paths\n",
    "print(\"MODEL_PATH:\", MODEL_PATH)\n",
    "print(\"ENCODER_PATH:\", ENCODER_PATH)\n",
    "print(\"DECODER_PATH:\", DECODER_PATH)\n",
    "print(\"RESULT_FILE_NAME:\", RESULT_FILE_NAME)\n",
    "print(\"HISTORY_FILE_NAME:\", HISTORY_FILE_NAME)\n",
    "print(\"ACC_PLOT_FILE_NAME:\", ACC_PLOT_FILE_NAME)\n",
    "print(\"LOSS_PLOT_FILE_NAME:\", LOSS_PLOT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from mediapipe.python.solutions.holistic import Holistic\n",
    "import time\n",
    "import keras\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Masking, GRU\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check for cuda availability\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0, \"No GPU available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Parses the split files first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tvb_hksl_split_parser():\n",
    "    def __init__(self, file: str):\n",
    "        self.file = file\n",
    "        self.train_info = pd.read_csv(self.file, delimiter=\"|\") \n",
    "        # extend the dataframe with extracted information\n",
    "        self.train_info[\"glosses_tokenized\"] = self.train_info[\"glosses\"].str.split(' ')\n",
    "        # self.train_info[\"date\"] = self.train_info[\"id\"].str.split('/').apply(lambda x: x[0])\n",
    "        self.train_info[\"frames\"] = self.train_info[\"id\"].str.split('/').apply(lambda x: x[1])\n",
    "        self.train_info[\"length\"] = self.train_info[\"frames\"].str.split('-').apply(lambda x: int(x[1]) - int(x[0]) + 1)\n",
    "        # add <START> and <END> tokens to the glosses\n",
    "        self.train_info[\"glosses_tokenized\"] = self.train_info[\"glosses_tokenized\"].apply(lambda x: [\"<START>\"] + x + [\"<END>\"])\n",
    "        \n",
    "\n",
    "    def get_train_id(self) -> pd.Series:\n",
    "        if os.name == \"nt\": # for windows system only\n",
    "            return self.train_info[\"id\"].str.replace(\"/\", \"\\\\\")\n",
    "        return self.train_info[\"id\"]\n",
    "\n",
    "    # def get_train_date(self) -> pd.Series:\n",
    "    #     return self.train_info[\"date\"]\n",
    "    \n",
    "    # def get_train_frames(self) -> pd.Series:\n",
    "    #     return self.train_info[\"frames\"]\n",
    "\n",
    "    # def get_train_length(self) -> pd.Series:\n",
    "    #     return self.train_info[\"length\"]\n",
    "\n",
    "    def get_train_glosses_tokenized(self) -> pd.Series:\n",
    "        return self.train_info[\"glosses_tokenized\"]\n",
    "\n",
    "    def get_max_length(self) -> int:\n",
    "        return self.train_info[\"length\"].max()\n",
    "\n",
    "    # removed bc it returns a duplicate, not by memory reference\n",
    "    # def get_full_info(self) -> pd.DataFrame:\n",
    "    #     return self.train_info\n",
    "    \n",
    "    def get_word_dict(self) -> dict:\n",
    "        word_dict = {}\n",
    "        for tokens in self.train_info[\"glosses_tokenized\"]:\n",
    "            for token in tokens:\n",
    "                if token not in word_dict:\n",
    "                    word_dict[token] = len(word_dict)\n",
    "        return word_dict\n",
    "\n",
    "    def rare_token_reduction(self, token_freq) -> None:\n",
    "        # create a dictionary of all tokens and their frequencies\n",
    "        # token_freq = {}\n",
    "        # for tokens in self.train_info[\"glosses_tokenized\"]:\n",
    "        #     for token in tokens:\n",
    "        #         if token in token_freq:\n",
    "        #             token_freq[token] += 1\n",
    "        #         else:\n",
    "        #             token_freq[token] = 1\n",
    "\n",
    "        # simpler approach: if any token has a frequence of < 5, replace that token with <UNK>\n",
    "        def replace_rare_tokens(tokens):\n",
    "            return [\"<UNK>\" if token_freq[token] < 5 else token for token in tokens]\n",
    "        self.train_info[\"glosses_tokenized\"] = self.train_info[\"glosses_tokenized\"].apply(replace_rare_tokens)\n",
    "\n",
    "    def rare_sample_reduction(self, token_freq) -> None:\n",
    "        # remove samples with words that satisfy token_freq[token] = 1\n",
    "        self.train_info = self.train_info[self.train_info[\"glosses_tokenized\"].apply(lambda x: any([token_freq[token] < 5 if token in token_freq else True for token in x]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the word dictionary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parser = tvb_hksl_split_parser(\"../dataset/tvb-hksl-news/split/train.csv\")\n",
    "test_parser = tvb_hksl_split_parser(\"../dataset/tvb-hksl-news/split/test.csv\")\n",
    "dev_parser = tvb_hksl_split_parser(\"../dataset/tvb-hksl-news/split/dev.csv\")\n",
    "\n",
    "# make a word dictionary\n",
    "word_dict = {}\n",
    "word_dict[\"<END>\"] = len(word_dict)\n",
    "word_dict[\"<START>\"] = len(word_dict)\n",
    "word_dict[\"<X>\"] = len(word_dict)\n",
    "word_dict[\"<BAD>\"] = len(word_dict)\n",
    "word_dict[\"<MUMBLE>\"] = len(word_dict)\n",
    "word_dict[\"<STOP>\"] = len(word_dict)\n",
    "# word_dict[\"<UNK>\"] = len(word_dict)\n",
    "\n",
    "for parser in [train_parser, test_parser, dev_parser]:\n",
    "    for glosses in parser.get_train_glosses_tokenized():\n",
    "        for word in glosses:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = len(word_dict)\n",
    "\n",
    "# save the word dictionary\n",
    "with open(\"../data/word_dict.json\", \"w\") as f:\n",
    "    json.dump(word_dict, f)\n",
    "# save reverse word dictionary\n",
    "reverse_word_dict = {v: k for k, v in word_dict.items()}\n",
    "with open(\"../data/reverse_word_dict.json\", \"w+\") as f:\n",
    "    json.dump(reverse_word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency map: word -> frequency\n",
    "token_freq = {}\n",
    "total_word_count = 0\n",
    "for k, v in word_dict.items():\n",
    "    token_freq[v] = 0\n",
    "for parser in [train_parser, test_parser, dev_parser]:\n",
    "    for glosses in parser.get_train_glosses_tokenized():\n",
    "        for word in glosses:\n",
    "            token_freq[word_dict[word]] += 1\n",
    "            total_word_count += 1\n",
    "print(token_freq)\n",
    "print(len(token_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a weighting list, where lower frequency words have higher weight\n",
    "\n",
    "# basic inverse frequency weighting\n",
    "# weighting_list = [1 / token_freq[word] for word in token_freq]\n",
    "# weighting_list = [x / max(weighting_list) for x in weighting_list]\n",
    "\n",
    "# tf-idf weighting\n",
    "tf_list = np.array([token_freq[word] / total_word_count for word in token_freq]) # freq / total for each word\n",
    "idf_list = np.log(len(token_freq) / tf_list) # log(total / freq ratio) for each word\n",
    "weighting_list = tf_list * idf_list \n",
    "\n",
    "# make sure that <START> and <END> have full weight\n",
    "weighting_list[word_dict[\"<START>\"]] = 1\n",
    "weighting_list[word_dict[\"<END>\"]] = 1\n",
    "print(weighting_list)\n",
    "\n",
    "# export weighting list\n",
    "# convert numpy array to list\n",
    "weighting_list = weighting_list.tolist()\n",
    "with open(\"../data/weighting_list.json\", \"w\") as f:\n",
    "    json.dump(weighting_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample preprocessing\n",
    "# train_parser.rare_sample_reduction(token_freq)\n",
    "# test_parser.rare_sample_reduction(token_freq)\n",
    "# dev_parser.rare_sample_reduction(token_freq)\n",
    "\n",
    "if MODE == \"train\":\n",
    "    actual_train_parser = train_parser\n",
    "elif MODE == \"dev\":\n",
    "    actual_train_parser = dev_parser\n",
    "\n",
    "# if a word in test_parser is not in dev_parser or train_parser, remove that sample\n",
    "# this is to prevent the model from predicting words that are not in the training set\n",
    "# supposedly, this should not happen, but just in case\n",
    "parser_word_dict = actual_train_parser.get_word_dict()\n",
    "test_parser.train_info = test_parser.train_info[test_parser.train_info[\"glosses_tokenized\"].apply(lambda x: all([word in parser_word_dict for word in x]))]\n",
    "    \n",
    "# assert that all words in test_parser are also in train_parser\n",
    "test_word_dict = test_parser.get_word_dict()\n",
    "assert all([word in parser_word_dict for word in test_word_dict])\n",
    "\n",
    "# finally, print the number of samples in each parser\n",
    "print(f\"train_parser: {len(train_parser.train_info)}\")\n",
    "print(f\"test_parser: {len(test_parser.train_info)}\")\n",
    "print(f\"dev_parser: {len(dev_parser.train_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate decoder input and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the dictionary, create one-hot vectors for each word\n",
    "# then create decoder inputs and targets\n",
    "all_input_found = False\n",
    "if CACHE_Y:\n",
    "    # attempt to load the cached data if it exists\n",
    "    path_requirements = [\n",
    "        f\"../cache/train_decoder_input.npy\",\n",
    "        f\"../cache/train_decoder_target.npy\",\n",
    "        f\"../cache/test_decoder_input.npy\",\n",
    "        f\"../cache/test_decoder_target.npy\",\n",
    "        f\"../cache/dev_decoder_input.npy\",\n",
    "        f\"../cache/dev_decoder_target.npy\"\n",
    "    ]\n",
    "\n",
    "    if all([os.path.exists(path) for path in path_requirements]):\n",
    "        train_decoder_input = np.load(f\"../cache/train_decoder_input.npy\", mmap_mode=\"r\")\n",
    "        train_decoder_target = np.load(f\"../cache/train_decoder_target.npy\", mmap_mode=\"r\")\n",
    "        test_decoder_input = np.load(f\"../cache/test_decoder_input.npy\", mmap_mode=\"r\")\n",
    "        test_decoder_target = np.load(f\"../cache/test_decoder_target.npy\", mmap_mode=\"r\")\n",
    "        dev_decoder_input = np.load(f\"../cache/dev_decoder_input.npy\", mmap_mode=\"r\")\n",
    "        dev_decoder_target = np.load(f\"../cache/dev_decoder_target.npy\", mmap_mode=\"r\")\n",
    "        all_input_found = True\n",
    "        print(\"All cached data found.\")\n",
    "    else: print(\"Cached data not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run if there is no cached data\n",
    "if not all_input_found:\n",
    "    print(\"Creating decoder inputs and targets\")\n",
    "    train_glosses = train_parser.get_train_glosses_tokenized()\n",
    "    dev_glosses = dev_parser.get_train_glosses_tokenized()\n",
    "    test_glosses = test_parser.get_train_glosses_tokenized()\n",
    "\n",
    "    # find max length of glosses\n",
    "    max_length_train = train_glosses.apply(len).max()\n",
    "    max_length_test = test_glosses.apply(len).max()\n",
    "    max_length_dev = dev_glosses.apply(len).max()\n",
    "    max_length = max(max_length_train, max_length_test, max_length_dev)\n",
    "    print(\"Max length of glosses:\", max_length)\n",
    "\n",
    "    def create_decoder_inputs_targets(glosses: pd.Series, word_dict: dict, max_length: int):\n",
    "        decoder_input = np.zeros((len(glosses), max_length, len(word_dict)))\n",
    "        decoder_target = np.zeros((len(glosses), max_length, len(word_dict)))\n",
    "        for i in range(len(glosses)):\n",
    "            for j in range(len(glosses[i])):\n",
    "                decoder_input[i, j, word_dict[glosses[i][j]]] = 1\n",
    "                if j > 0:\n",
    "                    decoder_target[i, j-1, word_dict[glosses[i][j]]] = 1\n",
    "        return decoder_input, decoder_target\n",
    "\n",
    "    train_decoder_input, train_decoder_target = create_decoder_inputs_targets(train_glosses, word_dict, max_length)\n",
    "    test_decoder_input, test_decoder_target = create_decoder_inputs_targets(test_glosses, word_dict, max_length)\n",
    "    dev_decoder_input, dev_decoder_target = create_decoder_inputs_targets(dev_glosses, word_dict, max_length)\n",
    "\n",
    "    if CACHE_Y:\n",
    "        np.save(\"../cache/train_decoder_input.npy\", train_decoder_input)\n",
    "        np.save(\"../cache/train_decoder_target.npy\", train_decoder_target)\n",
    "        np.save(\"../cache/test_decoder_input.npy\", test_decoder_input)\n",
    "        np.save(\"../cache/test_decoder_target.npy\", test_decoder_target)\n",
    "        np.save(\"../cache/dev_decoder_input.npy\", dev_decoder_input)\n",
    "        np.save(\"../cache/dev_decoder_target.npy\", dev_decoder_target)\n",
    "    \n",
    "        del train_decoder_input\n",
    "        del train_decoder_target\n",
    "        del test_decoder_input\n",
    "        del test_decoder_target\n",
    "        del dev_decoder_input\n",
    "        del dev_decoder_target\n",
    "    \n",
    "        train_decoder_input = np.load(\"../cache/train_decoder_input.npy\", mmap_mode=\"r\")\n",
    "        train_decoder_target = np.load(\"../cache/train_decoder_target.npy\", mmap_mode=\"r\")\n",
    "        test_decoder_input = np.load(\"../cache/test_decoder_input.npy\", mmap_mode=\"r\")\n",
    "        test_decoder_target = np.load(\"../cache/test_decoder_target.npy\", mmap_mode=\"r\")\n",
    "        dev_decoder_input = np.load(\"../cache/dev_decoder_input.npy\", mmap_mode=\"r\")\n",
    "        dev_decoder_target = np.load(\"../cache/dev_decoder_target.npy\", mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: print the first sample of the training data undecoded using argmax\n",
    "print(\"Training data:\")\n",
    "print(\"Input:\")\n",
    "print([reverse_word_dict[i] for i in np.argmax(train_decoder_input[0], axis=1)])\n",
    "print(\"Target:\")\n",
    "print([reverse_word_dict[i] for i in np.argmax(train_decoder_target[0], axis=1)])\n",
    "# counter = 0\n",
    "# while True:\n",
    "#     local_input_data = [reverse_word_dict[i] for i in np.argmax(train_decoder_input[counter], axis=1)]\n",
    "#     local_target_data = [reverse_word_dict[i] for i in np.argmax(train_decoder_target[counter], axis=1)]\n",
    "#     if \"<UNK>\" not in local_input_data:\n",
    "#         counter += 1\n",
    "#         continue\n",
    "#     print(\"Input:\")\n",
    "#     print(local_input_data)\n",
    "#     print(\"Target:\")\n",
    "#     print(local_target_data)\n",
    "#     break\n",
    "\n",
    "# DEBUG: print the shapes of all the data\n",
    "print(\"Train decoder input shape:\", train_decoder_input.shape)  \n",
    "print(\"Train decoder target shape:\", train_decoder_target.shape)\n",
    "print(\"Test decoder input shape:\", test_decoder_input.shape)\n",
    "print(\"Test decoder target shape:\", test_decoder_target.shape)\n",
    "print(\"Dev decoder input shape:\", dev_decoder_input.shape)\n",
    "print(\"Dev decoder target shape:\", dev_decoder_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extract the features of each video sample. For each extraction, we apply a random transformation to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_size(image, target_size=(1920, 1080)):\n",
    "    rows, cols, _ = image.shape\n",
    "    target_cols, target_rows = target_size\n",
    "    scale_factor = min(target_cols / cols, target_rows / rows)\n",
    "    new_cols = int(cols * scale_factor)\n",
    "    new_rows = int(rows * scale_factor)\n",
    "    resized_image = cv2.resize(image, (new_cols, new_rows), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    top = (target_rows - new_rows) // 2\n",
    "    bottom = target_rows - new_rows - top\n",
    "    left = (target_cols - new_cols) // 2\n",
    "    right = target_cols - new_cols - left\n",
    "    padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "def apply_random_transformation(image, angle, tx, ty, scale):\n",
    "    rows, cols, _ = image.shape\n",
    "\n",
    "    # rotation\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "    # translation\n",
    "    M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "    translated_image = cv2.warpAffine(rotated_image, M, (cols, rows))\n",
    "\n",
    "    # scaling\n",
    "    scaled_image = cv2.resize(translated_image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # consistency\n",
    "    final_image = cv2.resize(scaled_image, (cols, rows), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return final_image\n",
    "\n",
    "def preprocess_image(image, target_size=(1920, 1080), angle=0, tx=0, ty=0, scale=1):\n",
    "    padded_image = pad_to_size(image, target_size)\n",
    "    transformed_image = apply_random_transformation(padded_image, angle, tx, ty, scale)\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Here, we use `mediapipe` by google to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mediapipe_keypoints_index() -> list[int]:\n",
    "        \"\"\"\n",
    "        Returns the indices of the keypoints that we want to keep.\n",
    "        \n",
    "        For the third dimension, we only want to keep the coordinates of\n",
    "        - Pose\n",
    "        - Face border\n",
    "        - Lips\n",
    "        - Eyes\n",
    "        - Eyebrows\n",
    "        - Nose\n",
    "        - Face Oval (border of face)\n",
    "        - Left hand\n",
    "        - Right hand\n",
    "\n",
    "        This is because we want the keypoints to be robust, thus the facial features that are unique to each signer are discarded.\n",
    "\n",
    "        Reference: https://github.com/LearningnRunning/py_face_landmark_helper/blob/main/mediapipe_helper/config.py\n",
    "        Image: https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "        Related stack overflow post: https://stackoverflow.com/questions/74901522/can-mediapipe-specify-which-parts-of-the-face-mesh-are-the-lips-or-nose-or-eyes\n",
    "        \"\"\"\n",
    "        # pose with visibility\n",
    "        # POSE = list(range(0, 33*4))\n",
    "\n",
    "        # pose without visibility\n",
    "        POSE_UNPROCESSED = range(0, 33*4)\n",
    "        # POSE = [i for i in POSE_UNPROCESSED if i % 4 != 3]\n",
    "        # for x, y only\n",
    "        # discard Z due to documentation https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/holistic.md\n",
    "        POSE = [i for i in POSE_UNPROCESSED if i % 4 != 2 and i % 4 != 3]\n",
    "\n",
    "        # face\n",
    "        # NOTE: the following keypoint indices are HARD-CODED based on the visualization of the face mesh\n",
    "        # reference: https://github.com/LearningnRunning/py_face_landmark_helper/blob/main/mediapipe_helper/config.py\n",
    "        # image: https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "        # related stack overflow post: https://stackoverflow.com/questions/74901522/can-mediapipe-specify-which-parts-of-the-face-mesh-are-the-lips-or-nose-or-eyes\n",
    "        FACE_LIPS = [0, 267, 269, 270, 13, 14, 17, 402, 146, 405, 409, 415, 291, 37, 39, 40, 178, 308, 181, 310, 311, 312, 185, 314, 317, 318, 61, 191, 321, 324, 78, 80, 81, 82, 84, 87, 88, 91, 95, 375]\n",
    "        LEFT_EYE = [384, 385, 386, 387, 388, 390, 263, 362, 398, 466, 373, 374, 249, 380, 381, 382]\n",
    "        LEFT_EYEBROW = [293, 295, 296, 300, 334, 336, 276, 282, 283, 285]\n",
    "        RIGHT_EYE = [160, 33, 161, 163, 133, 7, 173, 144, 145, 246, 153, 154, 155, 157, 158, 159]\n",
    "        RIGHT_EYEBROW = [65, 66, 70, 105, 107, 46, 52, 53, 55, 63]\n",
    "        FACE_NOSE = [1, 2, 4, 5, 6, 19, 275, 278, 294, 168, 45, 48, 440, 64, 195, 197, 326, 327, 344, 220, 94, 97, 98, 115]\n",
    "        FACE_OVAL = [132, 389, 136, 10, 397, 400, 148, 149, 150, 21, 152, 284, 288, 162, 297, 172, 176, 54, 58, 323, 67, 454, 332, 338, 93, 356, 103, 361, 234, 109, 365, 379, 377, 378, 251, 127]\n",
    "        FACE_UNPROCESSED = [item + 33*4 for sublist in [FACE_LIPS, LEFT_EYE, LEFT_EYEBROW, RIGHT_EYE, RIGHT_EYEBROW, FACE_NOSE, FACE_OVAL] for item in sublist]\n",
    "        # face keypoints are in x, y, z format flattened, so we need to capture all x, y, z values\n",
    "        FACE = [i for j in range(0, len(FACE_UNPROCESSED), 3) for i in range(FACE_UNPROCESSED[j], FACE_UNPROCESSED[j] + 3)]\n",
    "        # for x, y only\n",
    "        # FACE = [i for j in range(0, len(FACE_UNPROCESSED), 3) for i in range(FACE_UNPROCESSED[j], FACE_UNPROCESSED[j] + 2)]\n",
    "\n",
    "        # hands\n",
    "        LEFT_HAND = list(range(33*4 + 468*3, 33*4 + 468*3 + 21*3))\n",
    "        RIGHT_HAND = list(range(33*4 + 468*3 + 21*3, 33*4 + 468*3 + 21*3 + 21*3))\n",
    "        # for x, y only\n",
    "        # LEFT_HAND = [i for i in list(range(33*4 + 468*3, 33*4 + 468*3 + 21*3)) if i % 3 != 2]\n",
    "        # RIGHT_HAND = [i for i in list(range(33*4 + 468*3 + 21*3, 33*4 + 468*3 + 21*3 + 21*3)) if i % 3 != 2]\n",
    "        KEYPOINTS_INDEX = POSE + FACE + LEFT_HAND + RIGHT_HAND\n",
    "        return KEYPOINTS_INDEX\n",
    "\n",
    "STATIC_KEYPOINTS_INDEX = get_mediapipe_keypoints_index() # saves time by not recalculating the indices\n",
    "\n",
    "def mediapipe_detection(frame, holistic):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = holistic.process(frame)\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    return result\n",
    "\n",
    "def mediapipe_extract_keypoints(result):\n",
    "    # print(len(result.pose_landmarks.landmark))\n",
    "    # print(len(result.face_landmarks.landmark))\n",
    "    # print(len(result.left_hand_landmarks.landmark))\n",
    "    # print(len(result.right_hand_landmarks.landmark))\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in result.pose_landmarks.landmark]).flatten() if result.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "    concat = np.concatenate([pose, face, lh, rh])\n",
    "    return concat[STATIC_KEYPOINTS_INDEX]\n",
    "\n",
    "def mediapipe_extract(frames, override_random = False): # frames is a list of frames\n",
    "    if override_random:\n",
    "        angle = 0\n",
    "        tx = 0\n",
    "        ty = 0\n",
    "        scale = 1\n",
    "    else:\n",
    "        # predefine random transformation parameters\n",
    "        angle = np.random.uniform(-10, 10)\n",
    "        tx = np.random.uniform(-100, 100)\n",
    "        ty = np.random.uniform(-60, 60)\n",
    "        scale = np.random.uniform(0.6, 1.2)\n",
    "    frames = [preprocess_image(frame, angle=angle, tx=tx, ty=ty, scale=scale) for frame in frames]\n",
    "    # one optimization done here is to use the same holistic object for all frames\n",
    "    # this way, the model only needs to be loaded once\n",
    "    # then keypoints can be tracked until not found\n",
    "    with Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=0) as holistic:\n",
    "        return [mediapipe_extract_keypoints(mediapipe_detection(frame, holistic)) for frame in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to true to test the mediapipe extraction\n",
    "if False:\n",
    "    sample_image = cv2.imread(\"../dataset/tvb-hksl-news/frames/2020-01-16/000453-000550/000453.jpg\")\n",
    "    sample_keypoints = mediapipe_extract([sample_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to true to test the image preprocessing\n",
    "if False:\n",
    "    import matplotlib.pyplot as plt\n",
    "    source_directory = \"../dataset/tvb-hksl-news/frames/2020-01-16/000453-000550\"\n",
    "    source_list_frames = [cv2.imread(os.path.join(source_directory, frame)) for frame in sorted(os.listdir(source_directory))]\n",
    "    angle = np.random.uniform(-30, 30)\n",
    "    tx = np.random.uniform(-100, 100)\n",
    "    ty = np.random.uniform(-50, 50)\n",
    "    scale = np.random.uniform(0.6, 1.2)\n",
    "    frames = [preprocess_image(frame, angle=angle, tx=tx, ty=ty, scale=scale) for frame in source_list_frames]\n",
    "    # output each frame in jupyter notebook\n",
    "    for frame in frames:\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoint Generator\n",
    "\n",
    "Create a generator by inheriting `keras.utils.Sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation: get the largest length of sequences of x\n",
    "train_max_length = train_parser.get_max_length()\n",
    "test_max_length = test_parser.get_max_length()\n",
    "dev_max_length = dev_parser.get_max_length()\n",
    "\n",
    "X_max_length = max(train_max_length, test_max_length, dev_max_length)\n",
    "print(\"Max length of sequences of X:\", X_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X, decoder_input, decoder_target, X_max_length, batch_size=32):\n",
    "        self.x = X\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_target = decoder_target\n",
    "        self.X_max_length = X_max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.x) // self.batch_size\n",
    "        return (len(self.x) + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def __preprocess_x__(self, dir_location):\n",
    "        # x is the train id. the directory of the frames is located at ../dataset/tvb-hksl-news/frames/{train_id}\n",
    "        source_directory = f\"../dataset/tvb-hksl-news/frames/{dir_location}\"\n",
    "        # print(sorted(os.listdir(source_directory)))\n",
    "        source_list_frames = [cv2.imread(os.path.join(source_directory, frame)) for frame in sorted(os.listdir(source_directory))]\n",
    "        return source_list_frames\n",
    "\n",
    "    def __getitem__(self, idx, override_random = False):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = min((idx + 1) * self.batch_size, len(self.x))\n",
    "\n",
    "        # edge case: start_idx = len(self.x), which means the previous batch was the last batch\n",
    "        # if start_idx == len(self.x):\n",
    "        #     return [None, None], None\n",
    "        if start_idx >= len(self.x):\n",
    "            raise IndexError(\"Index out of range for the generator\")\n",
    "\n",
    "        batch_x = self.x[start_idx:end_idx]\n",
    "        batch_decoder_input = self.decoder_input[start_idx:end_idx]\n",
    "        batch_decoder_target = self.decoder_target[start_idx:end_idx]\n",
    "        # batch_x = [self.__preprocess_x__(dir_location) for dir_location in batch_x]\n",
    "        # batch_x = [mediapipe_extract(frames) for frames in batch_x]\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            batch_x = list(executor.map(self.__preprocess_x__, batch_x))\n",
    "        partial_mediapipe_extract = partial(mediapipe_extract, override_random=override_random)\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            # TODO: research for a faster method than mediapipe\n",
    "            # batch_x = list(executor.map(mediapipe_extract, batch_x, override_random=[override_random] * len(batch_x)))\n",
    "            batch_x = list(executor.map(partial_mediapipe_extract, batch_x))\n",
    "\n",
    "        # # pad each sequence to the max length\n",
    "        batch_x = keras.preprocessing.sequence.pad_sequences(batch_x, maxlen=self.X_max_length, padding=\"post\", dtype=\"float32\")\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_decoder_input = np.array(batch_decoder_input)\n",
    "        batch_decoder_target = np.array(batch_decoder_target)\n",
    "        return (batch_x, batch_decoder_input), batch_decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "if False:\n",
    "    dev_generator = KeypointGenerator(dev_parser.get_train_id(), dev_decoder_input, dev_decoder_target, X_max_length, batch_size=10)\n",
    "    start = time.time()\n",
    "    dev_generator.__getitem__(32)\n",
    "    end = time.time()\n",
    "    print(\"Time taken:\", end - start)\n",
    "    del dev_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Create a sequence-to-sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration value is only used for pre-generating and caching the data\n",
    "if MODE == \"train\":\n",
    "    keypoint_generator = KeypointGenerator(train_parser.get_train_id(), train_decoder_input, train_decoder_target, X_max_length, batch_size=BATCH_SIZE)\n",
    "    iteration = 20\n",
    "elif MODE == \"dev\":\n",
    "    keypoint_generator = KeypointGenerator(dev_parser.get_train_id(), dev_decoder_input, dev_decoder_target, X_max_length, batch_size=BATCH_SIZE)\n",
    "    iteration = 10\n",
    "else: raise ValueError(\"Invalid mode\")\n",
    "\n",
    "x_dir = os.path.join(CACHE_DIR, \"x\")\n",
    "decoder_input_dir = os.path.join(CACHE_DIR, \"decoder_input\")\n",
    "decoder_target_dir = os.path.join(CACHE_DIR, \"decoder_target\")\n",
    "\n",
    "os.makedirs(x_dir, exist_ok=True)\n",
    "os.makedirs(decoder_input_dir, exist_ok=True)\n",
    "os.makedirs(decoder_target_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if CACHE_BATCH, generate the batched data and save them. this will cause the data to be uniform every epoch (kinda bad for learning), but speeds up training since no processing is needed in every epoch\n",
    "if CACHE_BATCH:\n",
    "    for i in range(iteration):\n",
    "        counter = 0\n",
    "        while True:\n",
    "            print(f\"Iteration {i}, Counter {counter}\")\n",
    "            # break if the generator is exhausted (i.e. output < batch_size)\n",
    "            (batch_x, batch_decoder_input), batch_decoder_target = keypoint_generator.__getitem__(counter)\n",
    "            if batch_x is None: # edge case only\n",
    "                print(\"Generator exhausted\")\n",
    "                break\n",
    "            file_name = f\"iteration_{i}_batch_{counter}.npy\"\n",
    "            np.save(os.path.join(x_dir, file_name), batch_x)\n",
    "            np.save(os.path.join(decoder_input_dir, file_name), batch_decoder_input)\n",
    "            np.save(os.path.join(decoder_target_dir, file_name), batch_decoder_target)\n",
    "            counter += 1\n",
    "            batch_x_len = len(batch_x)\n",
    "            if len(batch_x) < keypoint_generator.batch_size:\n",
    "                print(\"Generator exhausted\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also generate control data for testing\n",
    "if CACHE_BATCH:\n",
    "    counter = 0\n",
    "    while True:\n",
    "        print(f\"Control, Counter {counter}\")\n",
    "        (batch_x, batch_decoder_input), batch_decoder_target = keypoint_generator.__getitem__(counter, override_random=True)\n",
    "        if batch_x is None:\n",
    "            print(\"Generator exhausted\")\n",
    "            break\n",
    "        file_name = f\"control_batch_{counter}.npy\"\n",
    "        np.save(os.path.join(x_dir, file_name), batch_x)\n",
    "        np.save(os.path.join(decoder_input_dir, file_name), batch_decoder_input)\n",
    "        np.save(os.path.join(decoder_target_dir, file_name), batch_decoder_target)\n",
    "        counter += 1\n",
    "        batch_x_len = len(batch_x)\n",
    "        if len(batch_x) < keypoint_generator.batch_size:\n",
    "            print(\"Generator exhausted\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data and combine them into one\n",
    "class CachedKeypointGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, x_dir, decoder_input_dir, decoder_target_dir, batch_size=32):\n",
    "        self.x_dir = x_dir\n",
    "        self.decoder_input_dir = decoder_input_dir\n",
    "        self.decoder_target_dir = decoder_target_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.list_x_files = sorted(os.listdir(self.x_dir))\n",
    "        self.list_decoder_input_files = sorted(os.listdir(self.decoder_input_dir))\n",
    "        self.list_decoder_target_files = sorted(os.listdir(self.decoder_target_dir))\n",
    "        if NO_TRANSFORM:\n",
    "            # only keep files that start with \"control\"\n",
    "            self.list_x_files = [file for file in self.list_x_files if file.startswith(\"control\")]\n",
    "            self.list_decoder_input_files = [file for file in self.list_decoder_input_files if file.startswith(\"control\")]\n",
    "            self.list_decoder_target_files = [file for file in self.list_decoder_target_files if file.startswith(\"control\")]\n",
    "        self.total_files = len(self.list_x_files)\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_files\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        batch_x = np.load(os.path.join(self.x_dir, self.list_x_files[idx]), mmap_mode=\"r\")\n",
    "        batch_decoder_input = np.load(os.path.join(self.decoder_input_dir, self.list_decoder_input_files[idx]), mmap_mode=\"r\")\n",
    "        batch_decoder_target = np.load(os.path.join(self.decoder_target_dir, self.list_decoder_target_files[idx]), mmap_mode=\"r\")\n",
    "        if USE_WEIGHT:\n",
    "            # on the decoder input and target, apply the weighting list\n",
    "            batch_decoder_input = batch_decoder_input * weighting_list\n",
    "            batch_decoder_target = batch_decoder_target * weighting_list\n",
    "        return (batch_x, batch_decoder_input), batch_decoder_target\n",
    "\n",
    "# in case we want to use provided keypoints or old keypoints\n",
    "# for these, no preprocessing is done on the images\n",
    "class ProvidedKeypointGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, parser, decoder_input, decoder_target, X_max_length, batch_size=32):\n",
    "        self.parser: tvb_hksl_split_parser = parser\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_target = decoder_target\n",
    "        self.X_max_length = X_max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.parser.get_train_id()) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = min((idx + 1) * self.batch_size, len(self.parser.get_train_id()))\n",
    "\n",
    "        batch_x = self.parser.get_train_id()[start_idx:end_idx]\n",
    "        batch_decoder_input = self.decoder_input[start_idx:end_idx]\n",
    "        batch_decoder_target = self.decoder_target[start_idx:end_idx]\n",
    "        \n",
    "        keypoints_dir = \"../dataset/tvb-hksl-news/keypoints_mediapipe\"\n",
    "        batch_x = [np.load(os.path.join(keypoints_dir, f\"{train_id}.npy\"), mmap_mode=\"r\") for train_id in batch_x]\n",
    "        batch_x = keras.preprocessing.sequence.pad_sequences(batch_x, maxlen=self.X_max_length, padding=\"post\", dtype=\"float32\")\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_decoder_input = np.array(batch_decoder_input)\n",
    "        batch_decoder_target = np.array(batch_decoder_target)\n",
    "        if USE_WEIGHT:\n",
    "            # on the decoder input and target, apply the weighting list\n",
    "            batch_decoder_input = batch_decoder_input * weighting_list\n",
    "            batch_decoder_target = batch_decoder_target * weighting_list\n",
    "        return (batch_x, batch_decoder_input), batch_decoder_target\n",
    "\n",
    "\n",
    "if USE_CACHE_BATCH:\n",
    "    del keypoint_generator\n",
    "    keypoint_generator = CachedKeypointGenerator(x_dir, decoder_input_dir, decoder_target_dir, batch_size=BATCH_SIZE)\n",
    "    print(\"Cached generator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CACHE_BATCH:\n",
    "    print(\"Using cached data\")\n",
    "    len_features = len(keypoint_generator.__getitem__(0)[0][0][0][0])\n",
    "else:\n",
    "    # generate the first batch but with batch_size = 1 to get the length of the features\n",
    "    temp_keypoint_generator = KeypointGenerator(train_parser.get_train_id(), train_decoder_input, train_decoder_target, X_max_length, batch_size=1)\n",
    "    (temp_batch_x, temp_batch_decoder_input), temp_batch_decoder_target = temp_keypoint_generator.__getitem__(0)\n",
    "    len_features = len(temp_batch_x[0][0])\n",
    "    del temp_keypoint_generator\n",
    "    del temp_batch_x\n",
    "    del temp_batch_decoder_input\n",
    "    del temp_batch_decoder_target\n",
    "\n",
    "if RNN_MODE == \"LSTM\":\n",
    "    # encoder\n",
    "    encoder_input = Input(shape=(None, len_features))\n",
    "    encoder_mask = Masking()(encoder_input)\n",
    "    encoder_lstm = LSTM(LATENT_DIM, return_state=True, return_sequences=True, use_cudnn=False)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_mask)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # decoder\n",
    "    decoder_input = Input(shape=(None, len(word_dict)))\n",
    "    decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, use_cudnn=False)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(len(word_dict), activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "elif RNN_MODE == \"GRU\":\n",
    "    # encoder\n",
    "    encoder_input = Input(shape=(None, len_features))\n",
    "    encoder_mask = Masking()(encoder_input)\n",
    "    encoder_gru = GRU(LATENT_DIM, return_state=True, return_sequences=True, use_cudnn=False)\n",
    "    encoder_outputs, state_h, state_c = encoder_gru(encoder_mask)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # decoder\n",
    "    decoder_input = Input(shape=(None, len(word_dict)))\n",
    "    decoder_gru = GRU(LATENT_DIM, return_sequences=True, return_state=True, use_cudnn=False)\n",
    "    decoder_outputs, _, _ = decoder_gru(decoder_input, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(len(word_dict), activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "model.compile(optimizer=RMSprop(), loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print instance of keypoint generator\n",
    "print(keypoint_generator.__getitem__(0)[0][0].shape)\n",
    "\n",
    "model.fit(keypoint_generator, epochs=EPOCH)\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this code if you wish to extend the training by more epochs\n",
    "if False:\n",
    "    # model = keras.models.load_model(f\"../model/{MODE}_model.keras\")\n",
    "    model.load_weights(MODEL_PATH)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "        if isinstance(layer, LSTM) or isinstance(layer, GRU):\n",
    "            layer.use_cudnn = False\n",
    "    # we do not need to recompile since nothing else has changed\n",
    "    # model.compile(optimizer=RMSprop(), loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "    # model.summary()\n",
    "    \n",
    "    new_total_epoch = 512\n",
    "    assert new_total_epoch > EPOCH\n",
    "    model.fit(keypoint_generator, epochs=new_total_epoch, initial_epoch=EPOCH)\n",
    "    model.save(f\"../model/{MODEL_PATH}_extended_{new_total_epoch}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Here, we attempt to decode the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and create encoder and decoder models\n",
    "if True:\n",
    "    # model.load_weights(f\"../model/{MODEL_PATH}_extended_{new_total_epoch}.keras\")\n",
    "    model.load_weights(MODEL_PATH)\n",
    "\n",
    "    # list all the layers\n",
    "    # for i, layer in enumerate(model.layers):\n",
    "    #     print(i, layer.name)\n",
    "    \"\"\"\n",
    "    0 input_layer_12\n",
    "    1 input_layer_13\n",
    "    2 masking_2\n",
    "    3 lstm_4\n",
    "    4 lstm_5\n",
    "    5 dense_2\n",
    "    \"\"\"\n",
    "\n",
    "    # encoder_input = model.layers[0]\n",
    "    # encoder_lstm = model.layers[3]\n",
    "    # encoder_states = encoder_lstm.output\n",
    "\n",
    "    # decoder_input = model.layers[1]\n",
    "    # decoder_lstm = model.layers[4]\n",
    "    # decoder_dense = model.layers[5]\n",
    "\n",
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_input, initial_state=decoder_states_inputs) if RNN_MODE == \"LSTM\" else decoder_gru(decoder_input, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# save the encoder and decoder models\n",
    "encoder_model.save(ENCODER_PATH)\n",
    "decoder_model.save(DECODER_PATH)\n",
    "# model.save(f\"../model/{MODE}_encoder_{weighted_suffix}_{BATCH_SIZE}_{EPOCH}_{LATENT_DIM}_{TRIAL}_extended_{new_total_epoch}.keras\")\n",
    "# model.save(f\"../model/{MODE}_decoder_{weighted_suffix}_{BATCH_SIZE}_{EPOCH}_{LATENT_DIM}_{TRIAL}_extended_{new_total_epoch}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss against the epochs\n",
    "history = model.history\n",
    "plt.plot(history.history[\"categorical_accuracy\"])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show(ACC_PLOT_FILE_NAME)\n",
    "plt.savefig()\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "plt.savefig(LOSS_PLOT_FILE_NAME)\n",
    "\n",
    "# Save the history of the model\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "with open(HISTORY_FILE_NAME, mode=\"w+\") as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Use basic accuracy for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, len(word_dict)))\n",
    "    target_seq[0, 0, word_dict[\"<START>\"]] = 1\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_word_dict[sampled_token_index]\n",
    "        decoded_sentence.append(sampled_word)\n",
    "        if sampled_word == \"<END>\" or len(decoded_sentence) > X_max_length:\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, len(word_dict)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    encoder_model = keras.models.load_model(ENCODER_PATH)\n",
    "    decoder_model = keras.models.load_model(DECODER_PATH)\n",
    "    # encoder_model = keras.models.load_model(f\"../model/{MODE}_encoder_{weighted_suffix}_{BATCH_SIZE}_{EPOCH}_{LATENT_DIM}_{TRIAL}_extended_{new_total_epoch}.keras\")\n",
    "    # decoder_model = keras.models.load_model(f\"../model/{MODE}_decoder_{weighted_suffix}_{BATCH_SIZE}_{EPOCH}_{LATENT_DIM}_{TRIAL}_extended_{new_total_epoch}.keras\")\n",
    "    \n",
    "    # disable cudnn\n",
    "    for layer in encoder_model.layers:\n",
    "        if isinstance(layer, LSTM) or isinstance(layer, GRU):\n",
    "            layer.use_cudnn = False\n",
    "    for layer in decoder_model.layers:\n",
    "        if isinstance(layer, LSTM) or isinstance(layer, GRU):\n",
    "            layer.use_cudnn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on testing data\n",
    "test_generator = KeypointGenerator(test_parser.get_train_id(), test_decoder_input, test_decoder_target, X_max_length, batch_size=1)\n",
    "test_x_dir = os.path.join(CACHE_DIR, \"test_x\")\n",
    "os.makedirs(test_x_dir, exist_ok=True)\n",
    "\n",
    "testing_results = []\n",
    "\n",
    "with open(RESULT_FILE_NAME, \"w+\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"ID\", \"Decoded Sentence\", \"Target Sentence\", \"Accuracy\"])\n",
    "\n",
    "    for i in range(len(test_parser.get_train_id())):\n",
    "        if not os.path.exists(os.path.join(test_x_dir, f\"batch_{i}.npy\")) or not os.path.exists(os.path.join(test_x_dir, f\"batch_{i}_decoder_target.npy\")):\n",
    "            (batch_x, batch_decoder_input), batch_decoder_target = test_generator.__getitem__(i, override_random=True)\n",
    "            file_name = f\"batch_{i}.npy\"\n",
    "            np.save(os.path.join(test_x_dir, file_name), batch_x)\n",
    "            file_name = f\"batch_{i}_decoder_target.npy\"\n",
    "            np.save(os.path.join(test_x_dir, file_name), batch_decoder_target)\n",
    "        else:\n",
    "            batch_x = np.load(os.path.join(test_x_dir, f\"batch_{i}.npy\"), mmap_mode=\"r\")\n",
    "            batch_decoder_target = np.load(os.path.join(test_x_dir, f\"batch_{i}_decoder_target.npy\"), mmap_mode=\"r\")\n",
    "        # print(batch_x.shape)\n",
    "        # batch_x = np.load(os.path.join(test_x_dir, file_name), mmap_mode=\"r\")\n",
    "        decoder_sentence = decode_sequence(batch_x)\n",
    "        decoder_sentence = decoder_sentence[:decoder_sentence.index(\"<END>\")] if \"<END>\" in decoder_sentence else decoder_sentence\n",
    "        target = [reverse_word_dict[i] for i in np.argmax(batch_decoder_target[0], axis=1)]\n",
    "        target = target[:target.index(\"<END>\")] if \"<END>\" in target else target\n",
    "        # print(f\"ID: {test_parser.get_train_id()[i]}\")\n",
    "        # print(\"Decoded sentence:\", decoder_sentence)\n",
    "        # print(\"Target sentence:\", target)\n",
    "        # pad the shorter sentence with <BAD> tokens\n",
    "        if len(decoder_sentence) < len(target):\n",
    "            decoder_sentence += [\"<BAD>\"] * (len(target) - len(decoder_sentence))\n",
    "        elif len(decoder_sentence) > len(target):\n",
    "            target += [\"<BAD>\"] * (len(decoder_sentence) - len(target))\n",
    "        accuracy = sum([1 for i in range(len(target)) if decoder_sentence[i] == target[i]]) / len(target)\n",
    "        # print(\"Accuracy:\", accuracy)\n",
    "        # print(\"=====================================\", end=\"\\n\\n\")\n",
    "        result_row = [\n",
    "            test_parser.get_train_id()[i],\n",
    "            decoder_sentence,\n",
    "            target,\n",
    "            accuracy\n",
    "        ]\n",
    "        writer.writerow(result_row)\n",
    "        testing_results.append(result_row)\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the total accuracy\n",
    "test_acc_correct = 0\n",
    "test_acc_total = 0\n",
    "for row in testing_results:\n",
    "    for j in range(len(row[1])):\n",
    "        if row[1][j] == row[2][j]:\n",
    "            test_acc_correct += 1\n",
    "        test_acc_total += 1\n",
    "test_acc = test_acc_correct / test_acc_total\n",
    "print(\"Total accuracy on testing data:\", test_acc)\n",
    "avg_acc = sum([row[3] for row in testing_results]) / len(testing_results)\n",
    "print(\"Average accuracy on testing data:\", avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on training data\n",
    "training_results = []\n",
    "\n",
    "train_result_file_name = RESULT_FILE_NAME.replace(\".csv\", \"_train.csv\")\n",
    "\n",
    "batch_x_list = [x for x in sorted(os.listdir(x_dir)) if x.startswith(\"control\")]\n",
    "# print(len(batch_x_list))\n",
    "# print(batch_x_list)\n",
    "\n",
    "open(train_result_file_name, \"w\").close()\n",
    "\n",
    "with open(train_result_file_name, \"w+\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"ID\", \"Decoded Sentence\", \"Target Sentence\", \"Accuracy\"])\n",
    "\n",
    "    for batch in range(len(batch_x_list)):\n",
    "        batch_x = np.load(os.path.join(x_dir, batch_x_list[batch]), mmap_mode=\"r\")\n",
    "        batch_decoder_target = np.load(os.path.join(decoder_target_dir, batch_x_list[batch]), mmap_mode=\"r\")\n",
    "        batch_length = len(batch_x)\n",
    "        for i in range(batch_length):\n",
    "            local_x = np.array([batch_x[i]])\n",
    "            local_decoder_target = np.array(batch_decoder_target[i])\n",
    "\n",
    "            decoder_sentence = decode_sequence(local_x)\n",
    "            decoder_sentence = decoder_sentence[:decoder_sentence.index(\"<END>\")] if \"<END>\" in decoder_sentence else decoder_sentence\n",
    "            target = [reverse_word_dict[i] for i in np.argmax(local_decoder_target, axis=1)]\n",
    "            target = target[:target.index(\"<END>\")] if \"<END>\" in target else target\n",
    "            # pad the shorter sentence with <BAD> tokens\n",
    "            if len(decoder_sentence) < len(target):\n",
    "                decoder_sentence += [\"<BAD>\"] * (len(target) - len(decoder_sentence))\n",
    "            elif len(decoder_sentence) > len(target):\n",
    "                target += [\"<BAD>\"] * (len(decoder_sentence) - len(target))\n",
    "            accuracy = sum([1 for i in range(len(target)) if decoder_sentence[i] == target[i]]) / len(target)\n",
    "            result_row = [\n",
    "                train_parser.get_train_id()[batch * BATCH_SIZE + i],\n",
    "                decoder_sentence,\n",
    "                target,\n",
    "                accuracy\n",
    "            ]\n",
    "            writer.writerow(result_row)\n",
    "            training_results.append(result_row)\n",
    "            f.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign2gloss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
